{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvBClcuZHrealOU3do5CYi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeganT2004/ENG1-T17-Website/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "laI_1-5gwGSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e466a69-9c3b-4cb8-d320-9156e416c66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch: 1/50, step:   40/255] loss: 4.627\n",
            "[epoch: 1/50, step:   80/255] loss: 4.631\n",
            "[epoch: 1/50, step:  120/255] loss: 4.626\n",
            "[epoch: 1/50, step:  160/255] loss: 4.629\n",
            "[epoch: 1/50, step:  200/255] loss: 4.626\n",
            "[epoch: 1/50, step:  240/255] loss: 4.626\n",
            "[epoch: 2/50, step:   25/255] loss: 2.895\n",
            "[epoch: 2/50, step:   65/255] loss: 4.624\n",
            "[epoch: 2/50, step:  105/255] loss: 4.623\n",
            "[epoch: 2/50, step:  145/255] loss: 4.629\n",
            "[epoch: 2/50, step:  185/255] loss: 4.626\n",
            "[epoch: 2/50, step:  225/255] loss: 4.629\n",
            "[epoch: 3/50, step:   10/255] loss: 1.159\n",
            "[epoch: 3/50, step:   50/255] loss: 4.620\n",
            "[epoch: 3/50, step:   90/255] loss: 4.626\n",
            "[epoch: 3/50, step:  130/255] loss: 4.632\n",
            "[epoch: 3/50, step:  170/255] loss: 4.625\n",
            "[epoch: 3/50, step:  210/255] loss: 4.628\n",
            "[epoch: 3/50, step:  250/255] loss: 4.629\n",
            "[epoch: 4/50, step:   35/255] loss: 4.050\n",
            "[epoch: 4/50, step:   75/255] loss: 4.629\n",
            "[epoch: 4/50, step:  115/255] loss: 4.622\n",
            "[epoch: 4/50, step:  155/255] loss: 4.627\n",
            "[epoch: 4/50, step:  195/255] loss: 4.625\n",
            "[epoch: 4/50, step:  235/255] loss: 4.627\n",
            "[epoch: 5/50, step:   20/255] loss: 2.309\n",
            "[epoch: 5/50, step:   60/255] loss: 4.627\n",
            "[epoch: 5/50, step:  100/255] loss: 4.627\n",
            "[epoch: 5/50, step:  140/255] loss: 4.623\n",
            "[epoch: 5/50, step:  180/255] loss: 4.626\n",
            "[epoch: 5/50, step:  220/255] loss: 4.631\n",
            "[epoch: 6/50, step:    5/255] loss: 0.577\n",
            "[epoch: 6/50, step:   45/255] loss: 4.622\n",
            "[epoch: 6/50, step:   85/255] loss: 4.626\n",
            "[epoch: 6/50, step:  125/255] loss: 4.624\n",
            "[epoch: 6/50, step:  165/255] loss: 4.626\n",
            "[epoch: 6/50, step:  205/255] loss: 4.623\n",
            "[epoch: 6/50, step:  245/255] loss: 4.629\n",
            "[epoch: 7/50, step:   30/255] loss: 3.464\n",
            "[epoch: 7/50, step:   70/255] loss: 4.625\n",
            "[epoch: 7/50, step:  110/255] loss: 4.625\n",
            "[epoch: 7/50, step:  150/255] loss: 4.621\n",
            "[epoch: 7/50, step:  190/255] loss: 4.623\n",
            "[epoch: 7/50, step:  230/255] loss: 4.621\n",
            "[epoch: 8/50, step:   15/255] loss: 1.730\n",
            "[epoch: 8/50, step:   55/255] loss: 4.614\n",
            "[epoch: 8/50, step:   95/255] loss: 4.612\n",
            "[epoch: 8/50, step:  135/255] loss: 4.608\n",
            "[epoch: 8/50, step:  175/255] loss: 4.606\n",
            "[epoch: 8/50, step:  215/255] loss: 4.602\n",
            "[epoch: 8/50, step:  255/255] loss: 4.589\n",
            "[epoch: 9/50, step:   40/255] loss: 4.558\n",
            "[epoch: 9/50, step:   80/255] loss: 4.569\n",
            "[epoch: 9/50, step:  120/255] loss: 4.535\n",
            "[epoch: 9/50, step:  160/255] loss: 4.518\n",
            "[epoch: 9/50, step:  200/255] loss: 4.546\n",
            "[epoch: 9/50, step:  240/255] loss: 4.472\n",
            "[epoch: 10/50, step:   25/255] loss: 2.745\n",
            "[epoch: 10/50, step:   65/255] loss: 4.369\n",
            "[epoch: 10/50, step:  105/255] loss: 4.428\n",
            "[epoch: 10/50, step:  145/255] loss: 4.421\n",
            "[epoch: 10/50, step:  185/255] loss: 4.349\n",
            "[epoch: 10/50, step:  225/255] loss: 4.281\n",
            "[epoch: 11/50, step:   10/255] loss: 1.055\n",
            "[epoch: 11/50, step:   50/255] loss: 4.270\n",
            "[epoch: 11/50, step:   90/255] loss: 4.290\n",
            "[epoch: 11/50, step:  130/255] loss: 4.361\n",
            "[epoch: 11/50, step:  170/255] loss: 4.321\n",
            "[epoch: 11/50, step:  210/255] loss: 4.258\n",
            "[epoch: 11/50, step:  250/255] loss: 4.327\n",
            "[epoch: 12/50, step:   35/255] loss: 3.697\n",
            "[epoch: 12/50, step:   75/255] loss: 4.218\n",
            "[epoch: 12/50, step:  115/255] loss: 4.235\n",
            "[epoch: 12/50, step:  155/255] loss: 4.256\n",
            "[epoch: 12/50, step:  195/255] loss: 4.328\n",
            "[epoch: 12/50, step:  235/255] loss: 4.251\n",
            "[epoch: 13/50, step:   20/255] loss: 2.124\n",
            "[epoch: 13/50, step:   60/255] loss: 4.236\n",
            "[epoch: 13/50, step:  100/255] loss: 4.224\n",
            "[epoch: 13/50, step:  140/255] loss: 4.220\n",
            "[epoch: 13/50, step:  180/255] loss: 4.258\n",
            "[epoch: 13/50, step:  220/255] loss: 4.231\n",
            "[epoch: 14/50, step:    5/255] loss: 0.518\n",
            "[epoch: 14/50, step:   45/255] loss: 4.120\n",
            "[epoch: 14/50, step:   85/255] loss: 4.225\n",
            "[epoch: 14/50, step:  125/255] loss: 4.238\n",
            "[epoch: 14/50, step:  165/255] loss: 4.188\n",
            "[epoch: 14/50, step:  205/255] loss: 4.157\n",
            "[epoch: 14/50, step:  245/255] loss: 4.124\n",
            "[epoch: 15/50, step:   30/255] loss: 3.075\n",
            "[epoch: 15/50, step:   70/255] loss: 4.175\n",
            "[epoch: 15/50, step:  110/255] loss: 4.119\n",
            "[epoch: 15/50, step:  150/255] loss: 4.189\n",
            "[epoch: 15/50, step:  190/255] loss: 3.936\n",
            "[epoch: 15/50, step:  230/255] loss: 3.993\n",
            "[epoch: 16/50, step:   15/255] loss: 1.539\n",
            "[epoch: 16/50, step:   55/255] loss: 3.942\n",
            "[epoch: 16/50, step:   95/255] loss: 3.978\n",
            "[epoch: 16/50, step:  135/255] loss: 4.089\n",
            "[epoch: 16/50, step:  175/255] loss: 4.141\n",
            "[epoch: 16/50, step:  215/255] loss: 3.923\n",
            "[epoch: 16/50, step:  255/255] loss: 3.855\n",
            "[epoch: 17/50, step:   40/255] loss: 3.835\n",
            "[epoch: 17/50, step:   80/255] loss: 3.888\n",
            "[epoch: 17/50, step:  120/255] loss: 3.920\n",
            "[epoch: 17/50, step:  160/255] loss: 3.954\n",
            "[epoch: 17/50, step:  200/255] loss: 3.836\n",
            "[epoch: 17/50, step:  240/255] loss: 3.859\n",
            "[epoch: 18/50, step:   25/255] loss: 2.366\n",
            "[epoch: 18/50, step:   65/255] loss: 3.856\n",
            "[epoch: 18/50, step:  105/255] loss: 3.758\n",
            "[epoch: 18/50, step:  145/255] loss: 3.758\n",
            "[epoch: 18/50, step:  185/255] loss: 3.785\n",
            "[epoch: 18/50, step:  225/255] loss: 3.906\n",
            "[epoch: 19/50, step:   10/255] loss: 0.952\n",
            "[epoch: 19/50, step:   50/255] loss: 3.780\n",
            "[epoch: 19/50, step:   90/255] loss: 3.753\n",
            "[epoch: 19/50, step:  130/255] loss: 3.778\n",
            "[epoch: 19/50, step:  170/255] loss: 3.643\n",
            "[epoch: 19/50, step:  210/255] loss: 3.796\n",
            "[epoch: 19/50, step:  250/255] loss: 3.904\n",
            "[epoch: 20/50, step:   35/255] loss: 3.276\n",
            "[epoch: 20/50, step:   75/255] loss: 3.561\n",
            "[epoch: 20/50, step:  115/255] loss: 3.688\n",
            "[epoch: 20/50, step:  155/255] loss: 3.788\n",
            "[epoch: 20/50, step:  195/255] loss: 3.829\n",
            "[epoch: 20/50, step:  235/255] loss: 3.702\n",
            "[epoch: 21/50, step:   20/255] loss: 1.739\n",
            "[epoch: 21/50, step:   60/255] loss: 3.571\n",
            "[epoch: 21/50, step:  100/255] loss: 3.617\n",
            "[epoch: 21/50, step:  140/255] loss: 3.605\n",
            "[epoch: 21/50, step:  180/255] loss: 3.690\n",
            "[epoch: 21/50, step:  220/255] loss: 3.869\n",
            "[epoch: 22/50, step:    5/255] loss: 0.377\n",
            "[epoch: 22/50, step:   45/255] loss: 3.520\n",
            "[epoch: 22/50, step:   85/255] loss: 3.629\n",
            "[epoch: 22/50, step:  125/255] loss: 3.656\n",
            "[epoch: 22/50, step:  165/255] loss: 3.712\n",
            "[epoch: 22/50, step:  205/255] loss: 3.686\n",
            "[epoch: 22/50, step:  245/255] loss: 3.526\n",
            "[epoch: 23/50, step:   30/255] loss: 2.619\n",
            "[epoch: 23/50, step:   70/255] loss: 3.677\n",
            "[epoch: 23/50, step:  110/255] loss: 3.684\n",
            "[epoch: 23/50, step:  150/255] loss: 3.491\n",
            "[epoch: 23/50, step:  190/255] loss: 3.582\n",
            "[epoch: 23/50, step:  230/255] loss: 3.710\n",
            "[epoch: 24/50, step:   15/255] loss: 1.240\n",
            "[epoch: 24/50, step:   55/255] loss: 3.410\n",
            "[epoch: 24/50, step:   95/255] loss: 3.497\n",
            "[epoch: 24/50, step:  135/255] loss: 3.419\n",
            "[epoch: 24/50, step:  175/255] loss: 3.578\n",
            "[epoch: 24/50, step:  215/255] loss: 3.581\n",
            "[epoch: 24/50, step:  255/255] loss: 3.481\n",
            "[epoch: 25/50, step:   40/255] loss: 3.483\n",
            "[epoch: 25/50, step:   80/255] loss: 3.403\n",
            "[epoch: 25/50, step:  120/255] loss: 3.562\n",
            "[epoch: 25/50, step:  160/255] loss: 3.384\n",
            "[epoch: 25/50, step:  200/255] loss: 3.621\n",
            "[epoch: 25/50, step:  240/255] loss: 3.506\n",
            "[epoch: 26/50, step:   25/255] loss: 2.193\n",
            "[epoch: 26/50, step:   65/255] loss: 3.400\n",
            "[epoch: 26/50, step:  105/255] loss: 3.332\n",
            "[epoch: 26/50, step:  145/255] loss: 3.479\n",
            "[epoch: 26/50, step:  185/255] loss: 3.458\n",
            "[epoch: 26/50, step:  225/255] loss: 3.500\n",
            "[epoch: 27/50, step:   10/255] loss: 0.865\n",
            "[epoch: 27/50, step:   50/255] loss: 3.472\n",
            "[epoch: 27/50, step:   90/255] loss: 3.381\n",
            "[epoch: 27/50, step:  130/255] loss: 3.394\n",
            "[epoch: 27/50, step:  170/255] loss: 3.396\n",
            "[epoch: 27/50, step:  210/255] loss: 3.416\n",
            "[epoch: 27/50, step:  250/255] loss: 3.262\n",
            "[epoch: 28/50, step:   35/255] loss: 2.974\n",
            "[epoch: 28/50, step:   75/255] loss: 3.367\n",
            "[epoch: 28/50, step:  115/255] loss: 3.299\n",
            "[epoch: 28/50, step:  155/255] loss: 3.541\n",
            "[epoch: 28/50, step:  195/255] loss: 3.380\n",
            "[epoch: 28/50, step:  235/255] loss: 3.506\n",
            "[epoch: 29/50, step:   20/255] loss: 1.590\n",
            "[epoch: 29/50, step:   60/255] loss: 3.251\n",
            "[epoch: 29/50, step:  100/255] loss: 3.253\n",
            "[epoch: 29/50, step:  140/255] loss: 3.327\n",
            "[epoch: 29/50, step:  180/255] loss: 3.323\n",
            "[epoch: 29/50, step:  220/255] loss: 3.280\n",
            "[epoch: 30/50, step:    5/255] loss: 0.408\n",
            "[epoch: 30/50, step:   45/255] loss: 3.164\n",
            "[epoch: 30/50, step:   85/255] loss: 3.303\n",
            "[epoch: 30/50, step:  125/255] loss: 3.271\n",
            "[epoch: 30/50, step:  165/255] loss: 3.419\n",
            "[epoch: 30/50, step:  205/255] loss: 3.291\n",
            "[epoch: 30/50, step:  245/255] loss: 3.511\n",
            "[epoch: 31/50, step:   30/255] loss: 2.464\n",
            "[epoch: 31/50, step:   70/255] loss: 3.048\n",
            "[epoch: 31/50, step:  110/255] loss: 3.189\n",
            "[epoch: 31/50, step:  150/255] loss: 3.286\n",
            "[epoch: 31/50, step:  190/255] loss: 3.531\n",
            "[epoch: 31/50, step:  230/255] loss: 3.484\n",
            "[epoch: 32/50, step:   15/255] loss: 1.271\n",
            "[epoch: 32/50, step:   55/255] loss: 3.263\n",
            "[epoch: 32/50, step:   95/255] loss: 3.232\n",
            "[epoch: 32/50, step:  135/255] loss: 3.310\n",
            "[epoch: 32/50, step:  175/255] loss: 3.444\n",
            "[epoch: 32/50, step:  215/255] loss: 3.187\n",
            "[epoch: 32/50, step:  255/255] loss: 3.345\n",
            "[epoch: 33/50, step:   40/255] loss: 3.074\n",
            "[epoch: 33/50, step:   80/255] loss: 3.362\n",
            "[epoch: 33/50, step:  120/255] loss: 3.233\n",
            "[epoch: 33/50, step:  160/255] loss: 3.341\n",
            "[epoch: 33/50, step:  200/255] loss: 3.191\n",
            "[epoch: 33/50, step:  240/255] loss: 3.267\n",
            "[epoch: 34/50, step:   25/255] loss: 1.906\n",
            "[epoch: 34/50, step:   65/255] loss: 3.101\n",
            "[epoch: 34/50, step:  105/255] loss: 3.188\n",
            "[epoch: 34/50, step:  145/255] loss: 3.252\n",
            "[epoch: 34/50, step:  185/255] loss: 3.287\n",
            "[epoch: 34/50, step:  225/255] loss: 3.360\n",
            "[epoch: 35/50, step:   10/255] loss: 0.855\n",
            "[epoch: 35/50, step:   50/255] loss: 3.125\n",
            "[epoch: 35/50, step:   90/255] loss: 3.293\n",
            "[epoch: 35/50, step:  130/255] loss: 3.314\n",
            "[epoch: 35/50, step:  170/255] loss: 3.365\n",
            "[epoch: 35/50, step:  210/255] loss: 3.177\n",
            "[epoch: 35/50, step:  250/255] loss: 3.110\n",
            "[epoch: 36/50, step:   35/255] loss: 2.740\n",
            "[epoch: 36/50, step:   75/255] loss: 3.261\n",
            "[epoch: 36/50, step:  115/255] loss: 2.978\n",
            "[epoch: 36/50, step:  155/255] loss: 3.270\n",
            "[epoch: 36/50, step:  195/255] loss: 3.203\n",
            "[epoch: 36/50, step:  235/255] loss: 3.228\n",
            "[epoch: 37/50, step:   20/255] loss: 1.474\n",
            "[epoch: 37/50, step:   60/255] loss: 3.190\n",
            "[epoch: 37/50, step:  100/255] loss: 2.977\n",
            "[epoch: 37/50, step:  140/255] loss: 3.220\n",
            "[epoch: 37/50, step:  180/255] loss: 3.186\n",
            "[epoch: 37/50, step:  220/255] loss: 3.378\n",
            "[epoch: 38/50, step:    5/255] loss: 0.446\n",
            "[epoch: 38/50, step:   45/255] loss: 3.071\n",
            "[epoch: 38/50, step:   85/255] loss: 3.280\n",
            "[epoch: 38/50, step:  125/255] loss: 3.179\n",
            "[epoch: 38/50, step:  165/255] loss: 3.078\n",
            "[epoch: 38/50, step:  205/255] loss: 3.360\n",
            "[epoch: 38/50, step:  245/255] loss: 3.123\n",
            "[epoch: 39/50, step:   30/255] loss: 2.149\n",
            "[epoch: 39/50, step:   70/255] loss: 3.051\n",
            "[epoch: 39/50, step:  110/255] loss: 2.962\n",
            "[epoch: 39/50, step:  150/255] loss: 3.244\n",
            "[epoch: 39/50, step:  190/255] loss: 3.146\n",
            "[epoch: 39/50, step:  230/255] loss: 3.133\n",
            "[epoch: 40/50, step:   15/255] loss: 1.136\n",
            "[epoch: 40/50, step:   55/255] loss: 2.980\n",
            "[epoch: 40/50, step:   95/255] loss: 3.038\n",
            "[epoch: 40/50, step:  135/255] loss: 3.160\n",
            "[epoch: 40/50, step:  175/255] loss: 3.174\n",
            "[epoch: 40/50, step:  215/255] loss: 3.288\n",
            "[epoch: 40/50, step:  255/255] loss: 2.942\n",
            "[epoch: 41/50, step:   40/255] loss: 2.963\n",
            "[epoch: 41/50, step:   80/255] loss: 3.027\n",
            "[epoch: 41/50, step:  120/255] loss: 3.008\n",
            "[epoch: 41/50, step:  160/255] loss: 2.949\n",
            "[epoch: 41/50, step:  200/255] loss: 3.322\n",
            "[epoch: 41/50, step:  240/255] loss: 3.192\n",
            "[epoch: 42/50, step:   25/255] loss: 1.839\n",
            "[epoch: 42/50, step:   65/255] loss: 2.927\n",
            "[epoch: 42/50, step:  105/255] loss: 2.961\n",
            "[epoch: 42/50, step:  145/255] loss: 3.024\n",
            "[epoch: 42/50, step:  185/255] loss: 3.130\n",
            "[epoch: 42/50, step:  225/255] loss: 3.078\n",
            "[epoch: 43/50, step:   10/255] loss: 0.758\n",
            "[epoch: 43/50, step:   50/255] loss: 3.131\n",
            "[epoch: 43/50, step:   90/255] loss: 2.883\n",
            "[epoch: 43/50, step:  130/255] loss: 2.851\n",
            "[epoch: 43/50, step:  170/255] loss: 3.016\n",
            "[epoch: 43/50, step:  210/255] loss: 2.979\n",
            "[epoch: 43/50, step:  250/255] loss: 3.185\n",
            "[epoch: 44/50, step:   35/255] loss: 2.369\n",
            "[epoch: 44/50, step:   75/255] loss: 2.703\n",
            "[epoch: 44/50, step:  115/255] loss: 3.163\n",
            "[epoch: 44/50, step:  155/255] loss: 3.050\n",
            "[epoch: 44/50, step:  195/255] loss: 3.106\n",
            "[epoch: 44/50, step:  235/255] loss: 3.062\n",
            "[epoch: 45/50, step:   20/255] loss: 1.544\n",
            "[epoch: 45/50, step:   60/255] loss: 2.739\n",
            "[epoch: 45/50, step:  100/255] loss: 2.975\n",
            "[epoch: 45/50, step:  140/255] loss: 2.999\n",
            "[epoch: 45/50, step:  180/255] loss: 3.019\n",
            "[epoch: 45/50, step:  220/255] loss: 2.979\n",
            "[epoch: 46/50, step:    5/255] loss: 0.337\n",
            "[epoch: 46/50, step:   45/255] loss: 2.716\n",
            "[epoch: 46/50, step:   85/255] loss: 2.854\n",
            "[epoch: 46/50, step:  125/255] loss: 2.857\n",
            "[epoch: 46/50, step:  165/255] loss: 3.043\n",
            "[epoch: 46/50, step:  205/255] loss: 2.847\n",
            "[epoch: 46/50, step:  245/255] loss: 2.993\n",
            "[epoch: 47/50, step:   30/255] loss: 2.056\n",
            "[epoch: 47/50, step:   70/255] loss: 3.080\n",
            "[epoch: 47/50, step:  110/255] loss: 2.728\n",
            "[epoch: 47/50, step:  150/255] loss: 2.966\n",
            "[epoch: 47/50, step:  190/255] loss: 2.922\n",
            "[epoch: 47/50, step:  230/255] loss: 2.889\n",
            "[epoch: 48/50, step:   15/255] loss: 1.104\n",
            "[epoch: 48/50, step:   55/255] loss: 2.947\n",
            "[epoch: 48/50, step:   95/255] loss: 2.866\n",
            "[epoch: 48/50, step:  135/255] loss: 2.759\n",
            "[epoch: 48/50, step:  175/255] loss: 3.035\n",
            "[epoch: 48/50, step:  215/255] loss: 3.157\n",
            "[epoch: 48/50, step:  255/255] loss: 2.881\n",
            "[epoch: 49/50, step:   40/255] loss: 2.658\n",
            "[epoch: 49/50, step:   80/255] loss: 2.881\n",
            "[epoch: 49/50, step:  120/255] loss: 2.764\n",
            "[epoch: 49/50, step:  160/255] loss: 2.920\n",
            "[epoch: 49/50, step:  200/255] loss: 2.870\n",
            "[epoch: 49/50, step:  240/255] loss: 3.023\n",
            "[epoch: 50/50, step:   25/255] loss: 1.572\n",
            "[epoch: 50/50, step:   65/255] loss: 2.792\n",
            "[epoch: 50/50, step:  105/255] loss: 2.670\n",
            "[epoch: 50/50, step:  145/255] loss: 2.878\n",
            "[epoch: 50/50, step:  185/255] loss: 2.930\n",
            "[epoch: 50/50, step:  225/255] loss: 2.838\n",
            "Finished Training\n",
            "Accuracy: 14 %\n",
            "Accuracy for class: pink primrose is 55.0 %\n",
            "Accuracy for class: hard-leaved pocket orchid is 37.5 %\n",
            "Accuracy for class: canterbury bells is 5.0 %\n",
            "Accuracy for class: sweet pea is 0.0 %\n",
            "Accuracy for class: english marigold is 0.0 %\n",
            "Accuracy for class: tiger lily is 12.0 %\n",
            "Accuracy for class: moon orchid is 20.0 %\n",
            "Accuracy for class: bird of paradise is 47.7 %\n",
            "Accuracy for class: monkshood is 57.7 %\n",
            "Accuracy for class: globe thistle is 56.0 %\n",
            "Accuracy for class: snapdragon is 0.0 %\n",
            "Accuracy for class: colt's foot is 17.9 %\n",
            "Accuracy for class: king protea is 27.6 %\n",
            "Accuracy for class: spear thistle is 7.1 %\n",
            "Accuracy for class: yellow iris is 0.0 %\n",
            "Accuracy for class: globe-flower is 66.7 %\n",
            "Accuracy for class: purple coneflower is 15.4 %\n",
            "Accuracy for class: peruvian lily is 4.8 %\n",
            "Accuracy for class: balloon flower is 3.4 %\n",
            "Accuracy for class: giant white arum lily is 19.4 %\n",
            "Accuracy for class: fire lily is 30.0 %\n",
            "Accuracy for class: pincushion flower is 17.9 %\n",
            "Accuracy for class: fritillary is 9.9 %\n",
            "Accuracy for class: red ginger is 22.7 %\n",
            "Accuracy for class: grape hyacinth is 14.3 %\n",
            "Accuracy for class: corn poppy is 14.3 %\n",
            "Accuracy for class: prince of wales feathers is 5.0 %\n",
            "Accuracy for class: stemless gentian is 6.5 %\n",
            "Accuracy for class: artichoke is 10.3 %\n",
            "Accuracy for class: sweet william is 0.0 %\n",
            "Accuracy for class: carnation is 6.2 %\n",
            "Accuracy for class: garden phlox is 20.0 %\n",
            "Accuracy for class: love in the mist is 11.5 %\n",
            "Accuracy for class: mexican aster is 5.0 %\n",
            "Accuracy for class: alpine sea holly is 17.4 %\n",
            "Accuracy for class: ruby-lipped cattleya is 7.3 %\n",
            "Accuracy for class: cape flower is 60.2 %\n",
            "Accuracy for class: great masterwort is 8.3 %\n",
            "Accuracy for class: siam tulip is 4.8 %\n",
            "Accuracy for class: lenten rose is 6.4 %\n",
            "Accuracy for class: barbeton daisy is 0.0 %\n",
            "Accuracy for class: daffodil is 0.0 %\n",
            "Accuracy for class: sword lily is 2.7 %\n",
            "Accuracy for class: poinsettia is 27.4 %\n",
            "Accuracy for class: bolero deep blue is 0.0 %\n",
            "Accuracy for class: wallflower is 0.0 %\n",
            "Accuracy for class: marigold is 0.0 %\n",
            "Accuracy for class: buttercup is 0.0 %\n",
            "Accuracy for class: oxeye daisy is 58.6 %\n",
            "Accuracy for class: common dandelion is 0.0 %\n",
            "Accuracy for class: petunia is 18.9 %\n",
            "Accuracy for class: wild pansy is 55.4 %\n",
            "Accuracy for class: primula is 2.7 %\n",
            "Accuracy for class: sunflower is 36.6 %\n",
            "Accuracy for class: pelargonium is 58.8 %\n",
            "Accuracy for class: bishop of llandaff is 78.7 %\n",
            "Accuracy for class: gaura is 8.5 %\n",
            "Accuracy for class: geranium is 27.7 %\n",
            "Accuracy for class: orange dahlia is 36.2 %\n",
            "Accuracy for class: pink-yellow dahlia? is 1.1 %\n",
            "Accuracy for class: cautleya spicata is 90.0 %\n",
            "Accuracy for class: japanese anemone is 20.0 %\n",
            "Accuracy for class: black-eyed susan is 52.9 %\n",
            "Accuracy for class: silverbush is 6.2 %\n",
            "Accuracy for class: californian poppy is 15.9 %\n",
            "Accuracy for class: osteospermum is 61.0 %\n",
            "Accuracy for class: spring crocus is 4.5 %\n",
            "Accuracy for class: bearded iris is 0.0 %\n",
            "Accuracy for class: windflower is 29.4 %\n",
            "Accuracy for class: tree poppy is 4.8 %\n",
            "Accuracy for class: gazania is 48.3 %\n",
            "Accuracy for class: azalea is 2.6 %\n",
            "Accuracy for class: water lily is 1.1 %\n",
            "Accuracy for class: rose  is 0.7 %\n",
            "Accuracy for class: thorn apple is 0.0 %\n",
            "Accuracy for class: morning glory is 16.1 %\n",
            "Accuracy for class: passion flower is 0.0 %\n",
            "Accuracy for class: lotus is 18.8 %\n",
            "Accuracy for class: toad lily is 66.7 %\n",
            "Accuracy for class: anthurium is 11.8 %\n",
            "Accuracy for class: frangipani is 43.8 %\n",
            "Accuracy for class: clematis is 0.0 %\n",
            "Accuracy for class: hibiscus is 0.9 %\n",
            "Accuracy for class: columbine is 1.5 %\n",
            "Accuracy for class: desert-rose is 16.3 %\n",
            "Accuracy for class: tree mallow is 0.0 %\n",
            "Accuracy for class: magnolia is 16.3 %\n",
            "Accuracy for class: cyclamen is 2.2 %\n",
            "Accuracy for class: watercress is 0.0 %\n",
            "Accuracy for class: canna lily is 24.2 %\n",
            "Accuracy for class: hippeastrum is 1.8 %\n",
            "Accuracy for class: bee balm is 0.0 %\n",
            "Accuracy for class: ball moss is 3.8 %\n",
            "Accuracy for class: foxglove is 0.7 %\n",
            "Accuracy for class: bougainvillea is 1.9 %\n",
            "Accuracy for class: camellia is 0.0 %\n",
            "Accuracy for class: mallow is 2.2 %\n",
            "Accuracy for class: mexican petunia is 30.6 %\n",
            "Accuracy for class: bromelia is 0.0 %\n",
            "Accuracy for class: blanket flower is 48.3 %\n",
            "Accuracy for class: trumpet creeper is 2.6 %\n",
            "Accuracy for class: blackberry lily is 3.6 %\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = ( #preferred GPU usage\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "#hyperperameters\n",
        "numEpochs = 50\n",
        "learningRate = 0.01 #changes the model according to errors each time weights are updated\n",
        "\n",
        "\n",
        "trainTransform = transforms.Compose([\n",
        "      transforms.RandomRotation(30),\n",
        "      transforms.RandomResizedCrop(120),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]) #Normalise dataset between -1 and 1\n",
        "\n",
        "testTransform = transforms.Compose([\n",
        "      transforms.RandomRotation(30),\n",
        "      transforms.RandomResizedCrop(120),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "# Create datasets for training and testing, downloaded if not already\n",
        "trainSet = torchvision.datasets.Flowers102(root='./data', split = 'train', transform=trainTransform, download=True)\n",
        "testSet = torchvision.datasets.Flowers102(root='./data', split= \"test\", transform=testTransform, download=True)\n",
        "\n",
        "\n",
        "# Create data loaders for training and testing daasets\n",
        "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=4, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(testSet, batch_size=4, shuffle=False)\n",
        "\n",
        "classes = (\"pink primrose\",\n",
        "    \"hard-leaved pocket orchid\",\n",
        "    \"canterbury bells\",\n",
        "    \"sweet pea\",\n",
        "    \"english marigold\",\n",
        "    \"tiger lily\",\n",
        "    \"moon orchid\",\n",
        "    \"bird of paradise\",\n",
        "    \"monkshood\",\n",
        "    \"globe thistle\",\n",
        "    \"snapdragon\",\n",
        "    \"colt's foot\",\n",
        "    \"king protea\",\n",
        "    \"spear thistle\",\n",
        "    \"yellow iris\",\n",
        "    \"globe-flower\",\n",
        "    \"purple coneflower\",\n",
        "    \"peruvian lily\",\n",
        "    \"balloon flower\",\n",
        "    \"giant white arum lily\",\n",
        "    \"fire lily\",\n",
        "    \"pincushion flower\",\n",
        "    \"fritillary\",\n",
        "    \"red ginger\",\n",
        "    \"grape hyacinth\",\n",
        "    \"corn poppy\",\n",
        "    \"prince of wales feathers\",\n",
        "    \"stemless gentian\",\n",
        "    \"artichoke\",\n",
        "    \"sweet william\",\n",
        "    \"carnation\",\n",
        "    \"garden phlox\",\n",
        "    \"love in the mist\",\n",
        "    \"mexican aster\",\n",
        "    \"alpine sea holly\",\n",
        "    \"ruby-lipped cattleya\",\n",
        "    \"cape flower\",\n",
        "    \"great masterwort\",\n",
        "    \"siam tulip\",\n",
        "    \"lenten rose\",\n",
        "    \"barbeton daisy\",\n",
        "    \"daffodil\",\n",
        "    \"sword lily\",\n",
        "    \"poinsettia\",\n",
        "    \"bolero deep blue\",\n",
        "    \"wallflower\",\n",
        "    \"marigold\",\n",
        "    \"buttercup\",\n",
        "    \"oxeye daisy\",\n",
        "    \"common dandelion\",\n",
        "    \"petunia\",\n",
        "    \"wild pansy\",\n",
        "    \"primula\",\n",
        "    \"sunflower\",\n",
        "    \"pelargonium\",\n",
        "    \"bishop of llandaff\",\n",
        "    \"gaura\",\n",
        "    \"geranium\",\n",
        "    \"orange dahlia\",\n",
        "    \"pink-yellow dahlia?\",\n",
        "    \"cautleya spicata\",\n",
        "    \"japanese anemone\",\n",
        "    \"black-eyed susan\",\n",
        "    \"silverbush\",\n",
        "    \"californian poppy\",\n",
        "    \"osteospermum\",\n",
        "    \"spring crocus\",\n",
        "    \"bearded iris\",\n",
        "    \"windflower\",\n",
        "    \"tree poppy\",\n",
        "    \"gazania\",\n",
        "    \"azalea\",\n",
        "    \"water lily\",\n",
        "    \"rose\",\n",
        "    \"thorn apple\",\n",
        "    \"morning glory\",\n",
        "    \"passion flower\",\n",
        "    \"lotus\",\n",
        "    \"toad lily\",\n",
        "    \"anthurium\",\n",
        "    \"frangipani\",\n",
        "    \"clematis\",\n",
        "    \"hibiscus\",\n",
        "    \"columbine\",\n",
        "    \"desert-rose\",\n",
        "    \"tree mallow\",\n",
        "    \"magnolia\",\n",
        "    \"cyclamen\",\n",
        "    \"watercress\",\n",
        "    \"canna lily\",\n",
        "    \"hippeastrum\",\n",
        "    \"bee balm\",\n",
        "    \"ball moss\",\n",
        "    \"foxglove\",\n",
        "    \"bougainvillea\",\n",
        "    \"camellia\",\n",
        "    \"mallow\",\n",
        "    \"mexican petunia\",\n",
        "    \"bromelia\",\n",
        "    \"blanket flower\",\n",
        "    \"trumpet creeper\",\n",
        "    \"blackberry lily\")\n",
        "\n",
        "TrainIter = iter(trainLoader) #Picking a random assortment of training images\n",
        "images, labels = next(TrainIter)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(16, 26, 5)\n",
        "        self.fc1 = nn.Linear(3146, 1024) #breaking down the image until it is classified into one of the 102 categories\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, 102)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "eval = 40\n",
        "iteration=0\n",
        "\n",
        "for epoch in range(numEpochs):  # loop over the dataset multiple times\n",
        "\n",
        "    runningLoss = 0.0\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        iteration+=1\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        runningLoss += loss.item()\n",
        "        if iteration % eval == 0:\n",
        "            print(f'[epoch: {epoch + 1}/{numEpochs}, step:{i + 1:5d}/{len(trainLoader)}] loss: {runningLoss / 40:.3f}')\n",
        "            runningLoss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "PATH = './TrainedNet.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "testIter = iter(testLoader)\n",
        "images, labels = next(testIter)\n",
        "\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "outputs = net(images)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) #class with the highest prediction selected\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct // total} %')\n",
        "\n",
        "#preparing prediction calculation\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "     # print accuracy for each class\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "      accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "      print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    }
  ]
}